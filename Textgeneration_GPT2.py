# -*- coding: utf-8 -*-
"""GPTbbcnews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1duBveTdeOpHcQylhRQ7GtUl_RFliejHs
"""

!pip install transformers

import pandas as pd
import numpy as np
import random
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import torch.nn.functional as F
import csv

news = pd.read_csv('/content/drive/MyDrive/Textgen/news_uk.csv')
news.head()

data = news[news['description'].apply(lambda x: len(x.split(' ')) < 350)]

len(data)

class News(Dataset):

    def __init__(self, end_text, truncate=False, gpt2_type="gpt2", max_length=1024):

        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)
        self.news = []

        for item in data['description']:
          encoded_text = self.tokenizer.encode(f"<|{end_text}|>{item[:max_length]}<|endoftext|>")
          self.news.append(torch.tensor(encoded_text))

        if truncate:
            self.news = self.news[:30000]
        self.news_count = len(self.news)

    def __len__(self):
        return self.news_count

    def __getitem__(self, item):
        return self.news[item]

dataset = News(data['description'], truncate=True, gpt2_type="gpt2")

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

#Accumulated batch size (since GPT2 is so big)
def load_tensor(input_ten, remain_ten, max_seqlen):
    if remain_ten is None:
        return input_ten, True, None
    if input_ten.size()[1] + remain_ten.size()[1] > max_seqlen:
        return remain_ten, False, input_ten
    else:
        remain_ten = torch.cat([input_ten, remain_ten[:, 1:]], dim=1)
        return remain_ten, True, None

def train_gpt(data, model, tokenizer, batch_size=16, num_epochs=5, learning_rate=2e-5, max_seq_len=500, warmup_steps=250, gpt2_type="gpt2",
                output_dir=".", output_prefix="news"):


    accumulation_steps = 100
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.train()

    optimizer = AdamW(model.parameters(), lr=learning_rate)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1
    )

    news_loader = DataLoader(data, batch_size=batch_size, shuffle=True)
    loss = 0
    accumcount = 0
    input_tensor = None

    for i in range(num_epochs):
        print(f" Training epoch {i}")
        print(loss)
        for idx, x in tqdm(enumerate(news_loader)):
          input_tensor = input_tensor.to(device)
          output = model(input_tensor, labels=input_tensor)
          loss = output[0]
          loss.backward()

          if (accumulating_batch_count % batch_size) == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                model.zero_grad()

        accumulating_batch_count += 1
        input_tensor = None

    return model

model = train_gpt(dataset, model, tokenizer)

torch.save('/content/drive/MyDrive/Textgen/news_uk.pt')

model = torch.load('/content/drive/MyDrive/Textgen/news_uk.pt')

import torch
from tqdm import trange
import torch.nn.functional as F

def generate_gpt(model, tokenizer, prompt, prompt_count=1, word_length=200, top_p=0.8, temp=0.8):
    model.eval()
    num_generated = 0
    gen_list = []

    filter_value = -float("Inf")

    with torch.no_grad():

        for idx in trange(prompt_count):

            prompt_finished = False
            input_ten = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
            generated = input_ten.clone()  # Initialize generated with input_ten

            for i in range(word_length):
                output_ten = model(generated)
                logits = output_ten[0][:, -1, :] / (temp if temp > 0 else 1.0)

                # Apply top-p sampling
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0

                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[:, indices_to_remove] = filter_value

                # Sample the next token
                next_pred = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)

                generated = torch.cat((generated, next_pred), dim=1)

                if next_pred.item() in tokenizer.encode("<|endoftext|>"):
                    prompt_finished = True

                if prompt_finished:
                    num_generated = num_generated + 1

                    output_list = list(generated.squeeze().numpy())
                    output_text = tokenizer.decode(output_list)
                    gen_list.append(output_text)
                    break

            if not prompt_finished:
                output_list = list(generated.squeeze().numpy())
                output_text = f"{tokenizer.decode(output_list)}"
                gen_list.append(output_text)

    return gen_list

generate = generate_gpt(model.to('cpu'), tokenizer, 'UK government are')
generate

generate

generated = generate_gpt(model.to('cpu'), tokenizer, 'Boris Johnson is' )
generated

generated_new4 = generate(model.to('cpu'), tokenizer, "UK government are")
generated_new4

file_loc = './gen_4.txt'
with open(file_loc, "w") as file:
    for item in generated_new4:
        file.write(item
        )

generated_temp_1 = generate(model.to('cpu'), tokenizer, "Five things")
generated_temp_1

generated_old_model

generated_eig_twh

file_loc = './gen_fiv.txt'
with open(file_loc, "w") as file:
    for item in generated_fiv:
        file.write(item
        )

generated_eig

generated_fiv

#Function to generate multiple sentences. Test data should be a dataframe
def text_generation(test_data):
  generated_news = []
  for i in range(0, 10):#(len(test_data)):
    x = generate(model.to('cpu'), tokenizer, test_data['description'][i], entry_count=1)
    generated_news.append(x)
  return generated_news

test_set =pd.read_csv('./test_set.csv')
test_set.head()

generated_news = text_generation(test_set)

generated_news

len(generated_news)

path_file ='/content/drive/MyDrive/Textgen/GPT_gen2.txt'
with open(path_file, "w") as file:
    for item in generated_news:
        for sublist in item:
          file.write(sublist +"\n")

#Run the functions to generate the news
generated_news = text_generation(test_set)

generated_news

import statistics
from nltk.translate.bleu_score import sentence_bleu

scores=[]

for i in range(len(test_set)):
  reference = test_set['True_end_lyrics'][i]
  candidate = test_set['Generated_lyrics'][i]
  scores.append(sentence_bleu(reference, candidate))

statistics.mean(scores)

model = torch.load('/content/drive/MyDrive/Textgen/newsmodel1.pt')

model = train_model(dataset, model, tokenizer)
